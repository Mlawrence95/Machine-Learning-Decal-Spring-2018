{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "### For the second part of this project we will be implementing a simple Q-Learning algorithm on an RL environment called Cart Pole. The idea of Q-Learning is to try to estimate the expected future reward or Q-value of taking a certain action. Then at any given step we take the action with the most expected future reward.\n",
    "\n",
    "### In reinforcement learning, we refer to algorithms that attempt to solve environments as \"agents\", so in this part of the project we will be making a Deep Q Network Agent that will solve the Cart Pole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Setup the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Create The DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from collections import deque\n",
    "import random\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, env, replay_size=1000, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, gamma=0.99):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.model = self.build_model()\n",
    "        self.replay_buffer = deque(maxlen=replay_size)\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # TODO: add 2 dense layers each with 32 neurons, the input dim to the first\n",
    "        # layer should be the state size, also add relu activations, for both these layers\n",
    "        # Then add another Dense layer with num_actions neurons.\n",
    "        # Then use model.compile to compile the model with mse loss and an Adam optimizer\n",
    "        # with learning rate 0.001.\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def action(self, state):\n",
    "        # Whenever a random number between 0 and 1 is less than epsilon we want to return\n",
    "        # a random action. This means that with probability epsilon we return a random action.\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            #TODO: return random action here\n",
    "        # Now we want to use our model to get the q values\n",
    "        # HINT: we want to do prediction\n",
    "        q_values = ???\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def add_to_replay_buffer(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_batch_from_replay(self, batch_size):\n",
    "        # if we don't have enough samples in our replay buffer just return\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return False\n",
    "        # TODO: randomly sample batch_size samples from the replay buffer\n",
    "        # hint: use random.sample\n",
    "        minibatch = random.sample(self.replay_buffer, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_Qs = self.model.predict(next_state)[0]\n",
    "                # TODO: we want to add to our target GAMMA * max Q(next_state)\n",
    "                target += ???\n",
    "            # our target should only take into account the current action\n",
    "            # so we set all the Q values except the current action, to the \n",
    "            # current output of our model so that they get ignored in the loss function.\n",
    "            target_Qs = self.model.predict(state)\n",
    "            target_Qs[0][action] = target\n",
    "            self.model.fit(state, target_Qs, epochs=1, verbose=0)\n",
    "        \n",
    "        # Now we want to slowly decay how many random actions we take\n",
    "        # to do this we can multiply epsilon by our epsilon decay parameter\n",
    "        # each iteration\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            #TODO: YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "done = False\n",
    "batch_size = 32\n",
    "num_episodes = 800\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, agent.state_size])\n",
    "    \n",
    "    for t in range(200):\n",
    "        action = agent.action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else 100\n",
    "        next_state = np.reshape(next_state, [1, agent.state_size])\n",
    "        # TODO: add this sample to the replay buffer\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        # TODO: train on a batch from the replay buffer\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: set the agent's epsilon so that we don't take any random actions.\n",
    "for _ in range(10):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, agent.state_size])\n",
    "    total_reward = 0\n",
    "    for t in range(200):\n",
    "        action = agent.action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = np.reshape(next_state, [1, agent.state_size])\n",
    "        # TODO: if you want to see the rendered version of your agent running\n",
    "        # uncomment this line\n",
    "        #env.render()\n",
    "    print(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Writeup\n",
    "\n",
    "#### Now for the writeup portion write a paragraph of your understanding of how Deep Q Learning works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
